{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 1\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Helper Code\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "r='8' # last digit of roll number (B21CS098)\n",
    "data = pd.read_excel('./2_col_revised.xlsx', header=None, names=['x', 'y'])\n",
    "\n",
    "\n",
    "if 'r' in str(data.at[99, 'x']):   \n",
    "    data.at[99, 'x'] = str(data.at[99, 'x']).replace('r', r) \n",
    "\n",
    " \n",
    "data['x'] = pd.to_numeric(data['x'], errors='coerce')\n",
    "\n",
    " \n",
    "x = data['x'].values\n",
    "y = data['y'].values\n",
    "N = len(x)  \n",
    "\n",
    "beta_1, beta_2 = np.random.rand(2)\n",
    "learning_rate = 0.01"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimal β1 (slope): -0.2860070117454446\n",
      "Optimal β2 (intercept): 48.89937772163437\n",
      "Final loss: 136.63564159286378\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def loss_function(beta_1, beta_2, x, y):\n",
    "    predictions = beta_1 * x + beta_2\n",
    "    return (1 / (2 * N)) * np.sum((predictions - y) ** 2)\n",
    "\n",
    "\n",
    "def compute_gradients(beta_1, beta_2, x, y):\n",
    "    predictions = beta_1 * x + beta_2\n",
    "    error = predictions - y\n",
    "    gradient_beta_1 = (1 / N) * np.sum(error * x)\n",
    "    gradient_beta_2 = (1 / N) * np.sum(error)\n",
    "    return gradient_beta_1, gradient_beta_2\n",
    "\n",
    " \n",
    "tolerance = 0.01\n",
    "while True:\n",
    "     \n",
    "    grad_beta_1, grad_beta_2 = compute_gradients(beta_1, beta_2, x, y)\n",
    "    \n",
    "    \n",
    "    beta_1 -= learning_rate * grad_beta_1\n",
    "    beta_2 -= learning_rate * grad_beta_2\n",
    "    \n",
    "    \n",
    "    gradient_magnitude = np.sqrt(grad_beta_1 ** 2 + grad_beta_2 ** 2)\n",
    "    \n",
    "     \n",
    "    if gradient_magnitude < tolerance:\n",
    "        break\n",
    "\n",
    " \n",
    "print(\"Optimal β1 (slope):\", beta_1)\n",
    "print(\"Optimal β2 (intercept):\", beta_2)\n",
    "\n",
    " \n",
    "final_loss = loss_function(beta_1, beta_2, x, y)\n",
    "print(\"Final loss:\", final_loss)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimal β1 (slope): -0.19256512257533331\n",
      "Optimal β2 (intercept): 50.15776740816029\n",
      "Final loss: 138.17951821071102\n"
     ]
    }
   ],
   "source": [
    "def loss_function(beta_1, beta_2, x, y):\n",
    "    predictions = beta_1 * x + beta_2\n",
    "    return (1 / (2 * N)) * np.sum((predictions - y) ** 2)\n",
    "\n",
    "def compute_gradients(beta_1, beta_2, x_i, y_i):\n",
    "    prediction = beta_1 * x_i + beta_2\n",
    "    error = prediction - y_i\n",
    "    gradient_beta_1 = error * x_i\n",
    "    gradient_beta_2 = error\n",
    "    return gradient_beta_1, gradient_beta_2\n",
    "\n",
    "tolerance = 0.01\n",
    "max_iterations = 1000  \n",
    "\n",
    "for iteration in range(max_iterations):\n",
    "     \n",
    "    random_index = np.random.randint(N)\n",
    "    x_i = x[random_index]\n",
    "    y_i = y[random_index]\n",
    "    \n",
    "    grad_beta_1, grad_beta_2 = compute_gradients(beta_1, beta_2, x_i, y_i)\n",
    "    \n",
    "    beta_1 -= learning_rate * grad_beta_1\n",
    "    beta_2 -= learning_rate * grad_beta_2\n",
    "    \n",
    "    gradient_magnitude = np.sqrt(grad_beta_1 ** 2 + grad_beta_2 ** 2)\n",
    "    \n",
    "    if gradient_magnitude < tolerance:\n",
    "        print(f\"Converged after {iteration + 1} iterations.\")\n",
    "        break\n",
    "\n",
    "print(\"Optimal β1 (slope):\", beta_1)\n",
    "print(\"Optimal β2 (intercept):\", beta_2)\n",
    "\n",
    "final_loss = loss_function(beta_1, beta_2, x, y)\n",
    "print(\"Final loss:\", final_loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 3\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimal β1 (slope): -0.6152641644522665\n",
      "Optimal β2 (intercept): 49.831590878778165\n",
      "Final loss: 137.17189781407058\n"
     ]
    }
   ],
   "source": [
    "batch_size = 10  \n",
    "def loss_function(beta_1, beta_2, x, y):\n",
    "    predictions = beta_1 * x + beta_2\n",
    "    return (1 / (2 * N)) * np.sum((predictions - y) ** 2)\n",
    "\n",
    " \n",
    "def compute_gradients(beta_1, beta_2, x_batch, y_batch):\n",
    "    predictions = beta_1 * x_batch + beta_2\n",
    "    errors = predictions - y_batch\n",
    "    gradient_beta_1 = (1 / len(x_batch)) * np.sum(errors * x_batch)\n",
    "    gradient_beta_2 = (1 / len(x_batch)) * np.sum(errors)\n",
    "    return gradient_beta_1, gradient_beta_2\n",
    "\n",
    " \n",
    "tolerance = 0.01\n",
    "max_iterations = 1000  \n",
    "\n",
    "for iteration in range(max_iterations):\n",
    "    \n",
    "    indices = np.random.choice(N, batch_size, replace=False)\n",
    "    x_batch = x[indices]\n",
    "    y_batch = y[indices]\n",
    "    \n",
    "    \n",
    "    grad_beta_1, grad_beta_2 = compute_gradients(beta_1, beta_2, x_batch, y_batch)\n",
    "    \n",
    "   \n",
    "    beta_1 -= learning_rate * grad_beta_1\n",
    "    beta_2 -= learning_rate * grad_beta_2\n",
    "    \n",
    "    \n",
    "    gradient_magnitude = np.sqrt(grad_beta_1 ** 2 + grad_beta_2 ** 2)\n",
    "    \n",
    "    \n",
    "    if gradient_magnitude < tolerance:\n",
    "        print(f\"Converged after {iteration + 1} iterations.\")\n",
    "        break\n",
    "\n",
    "\n",
    "print(\"Optimal β1 (slope):\", beta_1)\n",
    "print(\"Optimal β2 (intercept):\", beta_2)\n",
    "\n",
    "\n",
    "final_loss = loss_function(beta_1, beta_2, x, y)\n",
    "print(\"Final loss:\", final_loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 4\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimal β0 (intercept): 5.895211123124055\n",
      "Optimal β1 (linear coefficient): 14.159292006378369\n",
      "Optimal β2 (quadratic coefficient): -1.0885589066907706\n",
      "Final loss: 148.25364779826793\n"
     ]
    }
   ],
   "source": [
    "beta_0, beta_1, beta_2 = np.random.rand(3)\n",
    "\n",
    "learning_rate = 0.001  \n",
    "\n",
    "def loss_function(beta_0, beta_1, beta_2, x, y):\n",
    "    predictions = beta_2 * x**2 + beta_1 * x + beta_0\n",
    "    return (1 / (2 * N)) * np.sum((predictions - y) ** 2)\n",
    "\n",
    "def compute_gradients(beta_0, beta_1, beta_2, x, y):\n",
    "    predictions = beta_2 * x**2 + beta_1 * x + beta_0\n",
    "    error = predictions - y\n",
    "    gradient_beta_0 = (1 / N) * np.sum(error)\n",
    "    gradient_beta_1 = (1 / N) * np.sum(error * x)\n",
    "    gradient_beta_2 = (1 / N) * np.sum(error * x**2)\n",
    "    return gradient_beta_0, gradient_beta_1, gradient_beta_2\n",
    "\n",
    "tolerance = 0.01\n",
    "max_iterations = 1000  \n",
    "\n",
    "for iteration in range(max_iterations):\n",
    "    grad_beta_0, grad_beta_1, grad_beta_2 = compute_gradients(beta_0, beta_1, beta_2, x, y)\n",
    "    beta_0 -= learning_rate * grad_beta_0\n",
    "    beta_1 -= learning_rate * grad_beta_1\n",
    "    beta_2 -= learning_rate * grad_beta_2\n",
    "    \n",
    "    gradient_magnitude = np.sqrt(grad_beta_0 ** 2 + grad_beta_1 ** 2 + grad_beta_2 ** 2)\n",
    "    \n",
    " \n",
    "    if gradient_magnitude < tolerance:\n",
    "        print(f\"Converged after {iteration + 1} iterations.\")\n",
    "        break\n",
    "\n",
    "print(\"Optimal β0 (intercept):\", beta_0)\n",
    "print(\"Optimal β1 (linear coefficient):\", beta_1)\n",
    "print(\"Optimal β2 (quadratic coefficient):\", beta_2)\n",
    "\n",
    "final_loss = loss_function(beta_0, beta_1, beta_2, x, y)\n",
    "print(\"Final loss:\", final_loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 5 and Question 6 are same as Question 2 and Question 3\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 7\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SGD Test Accuracy: 51.30%\n",
      "Mini-Batch Gradient Descent Test Accuracy: 47.40%\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "data = pd.read_csv('./diabetes.csv')\n",
    "\n",
    "data['outcome'] = data['outcome'].apply(lambda x: 1 if x == 'TRUE' else 0)   \n",
    "\n",
    "\n",
    "X = data.drop(columns=['outcome'])\n",
    "y = data['outcome']\n",
    "\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_scaled, y, test_size=0.2, random_state=42)\n",
    "\n",
    "\n",
    "def sigmoid(z):\n",
    "    return 1 / (1 + np.exp(-z))\n",
    "\n",
    "\n",
    "def loss_function(X, y, beta):\n",
    "    m = len(y)\n",
    "    predictions = sigmoid(np.dot(X, beta))\n",
    "    return -np.mean(y * np.log(predictions) + (1 - y) * np.log(1 - predictions))\n",
    "\n",
    "\n",
    "def compute_gradients(X, y, beta):\n",
    "    m = len(y)\n",
    "    predictions = sigmoid(np.dot(X, beta))\n",
    "    gradients = np.dot(X.T, (predictions - y)) / m\n",
    "    return gradients\n",
    "\n",
    "# (i) Stochastic Gradient Descent (SGD)\n",
    "def logistic_regression_sgd(X_train, y_train, learning_rate=0.01, tolerance=0.01, max_iterations=100):\n",
    "    m, n = X_train.shape\n",
    "    beta = np.zeros(n)  \n",
    "    for iteration in range(max_iterations):\n",
    "        for i in range(m):\n",
    "            xi = X_train[i:i+1]\n",
    "            yi = y_train.iloc[i:i+1]\n",
    "            \n",
    "            gradients = compute_gradients(xi, yi, beta)\n",
    "            beta -= learning_rate * gradients\n",
    "        \n",
    "        loss = loss_function(X_train, y_train, beta)\n",
    "        if loss < tolerance:\n",
    "            print(f\"SGD converged after {iteration + 1} iterations.\")\n",
    "            break\n",
    "\n",
    "    return beta\n",
    "\n",
    "# (ii) Mini-Batch Gradient Descent (with batch size of 10)\n",
    "def logistic_regression_mini_batch(X_train, y_train, batch_size=10, learning_rate=0.01, tolerance=0.01, max_iterations=100):\n",
    "    m, n = X_train.shape\n",
    "    beta = np.zeros(n)   \n",
    "    for iteration in range(max_iterations):\n",
    "        indices = np.random.permutation(m)\n",
    "        X_train_shuffled = X_train[indices]\n",
    "        y_train_shuffled = y_train.iloc[indices]\n",
    "\n",
    "        for i in range(0, m, batch_size):\n",
    "            Xi = X_train_shuffled[i:i+batch_size]\n",
    "            yi = y_train_shuffled.iloc[i:i+batch_size]\n",
    "            \n",
    "            gradients = compute_gradients(Xi, yi, beta)\n",
    "            beta -= learning_rate * gradients\n",
    "        \n",
    "        loss = loss_function(X_train, y_train, beta)\n",
    "        if loss < tolerance:\n",
    "            print(f\"Mini-Batch Gradient Descent converged after {iteration + 1} iterations.\")\n",
    "            break\n",
    "\n",
    "    return beta\n",
    "\n",
    "beta_sgd = logistic_regression_sgd(X_train, y_train, learning_rate=0.01)\n",
    "\n",
    "\n",
    "beta_mini_batch = logistic_regression_mini_batch(X_train, y_train, batch_size=10, learning_rate=0.01)\n",
    "\n",
    "\n",
    "def predict(X, beta):\n",
    "    return sigmoid(np.dot(X, beta)) >= 0.5\n",
    "\n",
    "\n",
    "y_pred_sgd = predict(X_test, beta_sgd)\n",
    "accuracy_sgd = np.mean(y_pred_sgd == y_test)\n",
    "print(f\"SGD Test Accuracy: {accuracy_sgd * 100:.2f}%\")\n",
    "\n",
    "\n",
    "y_pred_mini_batch = predict(X_test, beta_mini_batch)\n",
    "accuracy_mini_batch = np.mean(y_pred_mini_batch == y_test)\n",
    "print(f\"Mini-Batch Gradient Descent Test Accuracy: {accuracy_mini_batch * 100:.2f}%\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "omllabs",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
