{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 1\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### L1 Regularization\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Solution with fixed step size: [1.49999928 1.49999928]\n",
      "Solution with variable step size: [1.40186916 1.40186916]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "def soft_thresholding(x, alpha):\n",
    "    return np.sign(x) * np.maximum(np.abs(x) - alpha, 0)\n",
    "\n",
    "def proximal_gradient_method(initial_x, max_iterations=1000, alpha_type=\"fixed\", r=1, tol=1e-6):\n",
    "    x = np.array(initial_x, dtype=float)\n",
    "    history = [x.copy()]\n",
    "    \n",
    "    for k in range(max_iterations):\n",
    "        if alpha_type == \"fixed\":\n",
    "            alpha_k = 1/2\n",
    "        elif alpha_type == \"variable\":\n",
    "            alpha_k = 1 / (r + k)\n",
    "        else:\n",
    "            raise ValueError(\"alpha_type must be 'fixed' or 'variable'\")\n",
    "        \n",
    "        # Gradient of the smooth part\n",
    "        gradient_g = np.array([x[0] - 2, x[1] - 2])\n",
    "        \n",
    "        # Gradient step\n",
    "        x_gradient_step = x - alpha_k * gradient_g\n",
    "        \n",
    "        # Proximal step (using soft-thresholding for L1 norm)\n",
    "        x_next = soft_thresholding(x_gradient_step, alpha_k * 0.5)\n",
    "        \n",
    "        # Check for convergence\n",
    "        if np.linalg.norm(x_next - x) < tol:\n",
    "            break\n",
    "        \n",
    "        # Update x\n",
    "        x = x_next\n",
    "        history.append(x.copy())\n",
    "    \n",
    "    return x, history\n",
    "\n",
    "# Parameters\n",
    "initial_x = [0.0, 0.0]  # Initial guess\n",
    "r = 8  # Example last digit of roll number\n",
    "\n",
    "# Solve with fixed step size\n",
    "solution_fixed, history_fixed = proximal_gradient_method(initial_x, max_iterations=100, alpha_type=\"fixed\", r=r)\n",
    "\n",
    "# Solve with variable step size\n",
    "solution_variable, history_variable = proximal_gradient_method(initial_x, max_iterations=100, alpha_type=\"variable\", r=r)\n",
    "\n",
    "# Results\n",
    "print(\"Solution with fixed step size:\", solution_fixed)\n",
    "print(\"Solution with variable step size:\", solution_variable)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### L2 Regularization\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Solution with fixed step size: [1.33333276 1.33333276]\n",
      "Solution with variable step size: [1.30993379 1.30993379]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "def l2_proximal_operator(x, alpha):\n",
    "    \"\"\"\n",
    "    Proximal operator for L2 regularization (ridge)\n",
    "    For L2 norm, the proximal operator is: x / (1 + alpha)\n",
    "    \"\"\"\n",
    "    return x / (1 + alpha)\n",
    "\n",
    "def proximal_gradient_method_l2(initial_x, max_iterations=1000, alpha_type=\"fixed\", r=1, tol=1e-6):\n",
    "    x = np.array(initial_x, dtype=float)\n",
    "    history = [x.copy()]\n",
    "    \n",
    "    for k in range(max_iterations):\n",
    "        if alpha_type == \"fixed\":\n",
    "            alpha_k = 1/2\n",
    "        elif alpha_type == \"variable\":\n",
    "            alpha_k = 1 / (r + k)\n",
    "        else:\n",
    "            raise ValueError(\"alpha_type must be 'fixed' or 'variable'\")\n",
    "        \n",
    "        # Gradient of the smooth part (quadratic loss)\n",
    "        gradient_g = np.array([x[0] - 2, x[1] - 2])\n",
    "        \n",
    "        # Gradient step\n",
    "        x_gradient_step = x - alpha_k * gradient_g\n",
    "        \n",
    "        # Proximal step (using L2 proximal operator)\n",
    "        # The 0.5 factor is the regularization parameter (similar to your original code)\n",
    "        x_next = l2_proximal_operator(x_gradient_step, alpha_k * 0.5)\n",
    "        \n",
    "        # Check for convergence\n",
    "        if np.linalg.norm(x_next - x) < tol:\n",
    "            break\n",
    "        \n",
    "        # Update x\n",
    "        x = x_next\n",
    "        history.append(x.copy())\n",
    "    \n",
    "    return x, history\n",
    "\n",
    "# Parameters\n",
    "initial_x = [0.0, 0.0]  # Initial guess\n",
    "r = 8  # Example last digit of roll number\n",
    "\n",
    "# Solve with fixed step size\n",
    "solution_fixed, history_fixed = proximal_gradient_method_l2(initial_x, max_iterations=100, alpha_type=\"fixed\", r=r)\n",
    "\n",
    "# Solve with variable step size\n",
    "solution_variable, history_variable = proximal_gradient_method_l2(initial_x, max_iterations=100, alpha_type=\"variable\", r=r)\n",
    "\n",
    "# Results\n",
    "print(\"Solution with fixed step size:\", solution_fixed)\n",
    "print(\"Solution with variable step size:\", solution_variable)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 2\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### L1 Regularization\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Coefficients a: [-0.20587412 -0.45864919]\n",
      "Intercept beta: 4.7441788010967956e-07\n",
      "Solution is: [-2.05874121e-01 -4.58649186e-01  4.74417880e-07]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "def soft_thresholding(x, alpha):\n",
    "    return np.sign(x) * np.maximum(np.abs(x) - alpha, 0)\n",
    "\n",
    "def compute_step_size(A):\n",
    "    eigvals = np.linalg.eigvalsh(A.T @ A)\n",
    "    max_eigenvalue = np.max(eigvals)\n",
    "    # Step size alpha_k\n",
    "    alpha_k = 1 / (max_eigenvalue / 2 + 0.5)\n",
    "    return alpha_k\n",
    "\n",
    "def proximal_gradient_method(A, y, lambda_val, max_iterations=1000, tol=1e-6):\n",
    "    # Initialize x\n",
    "    x = np.zeros(A.shape[1])\n",
    "    history = [x.copy()]\n",
    "\n",
    "    # Compute the step size\n",
    "    alpha_k = compute_step_size(A)\n",
    "\n",
    "    for k in range(max_iterations):\n",
    "        # Gradient step\n",
    "        gradient = A.T @ (A @ x - y)\n",
    "        x_gradient_step = x - alpha_k * gradient\n",
    "\n",
    "        # Proximal step (soft-thresholding for L1 norm)\n",
    "        x_next = soft_thresholding(x_gradient_step, alpha_k * lambda_val)\n",
    "\n",
    "        # Check for convergence\n",
    "        if np.linalg.norm(x_next - x) < tol:\n",
    "            break\n",
    "\n",
    "        # Update x\n",
    "        x = x_next\n",
    "        history.append(x.copy())\n",
    "\n",
    "    return x, history\n",
    "\n",
    "\n",
    "data=pd.read_csv('./train_a.csv')\n",
    "\n",
    "A = data[['Value1','Value2']].values\n",
    "y = data['Result'].values\n",
    "\n",
    "# Add a column of ones to A for the intercept term\n",
    "A = np.hstack([A, np.ones((A.shape[0], 1))])\n",
    "\n",
    "\n",
    "R = 8  # last digit of roll number\n",
    "lambda_val = abs(R / 10 - 5)\n",
    "\n",
    "# Solve the optimization problem\n",
    "solution, history = proximal_gradient_method(A, y, lambda_val)\n",
    "\n",
    "# Extract coefficients and intercept\n",
    "a = solution[:2]\n",
    "beta = solution[2]\n",
    "\n",
    "# Output the results\n",
    "print(\"Coefficients a:\", a)\n",
    "print(\"Intercept beta:\", beta)\n",
    "print(\"Solution is:\", solution)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### L2 Regularization\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Coefficients a: [-0.20591411 -0.45863131]\n",
      "Intercept beta: -4.741042437662977e-06\n",
      "Solution is: [-2.05914109e-01 -4.58631309e-01 -4.74104244e-06]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "def compute_step_size(A):\n",
    "    eigvals = np.linalg.eigvalsh(A.T @ A)\n",
    "    max_eigenvalue = np.max(eigvals)\n",
    "    # Step size alpha_k\n",
    "    alpha_k = 1 / (max_eigenvalue / 2 + 0.5)\n",
    "    return alpha_k\n",
    "\n",
    "def proximal_gradient_method_l2(A, y, lambda_val, max_iterations=1000, tol=1e-6):\n",
    "    # Initialize x\n",
    "    x = np.zeros(A.shape[1])\n",
    "    history = [x.copy()]\n",
    "\n",
    "    # Compute the step size\n",
    "    alpha_k = compute_step_size(A)\n",
    "\n",
    "    for k in range(max_iterations):\n",
    "        # Gradient of the smooth part (least squares loss + L2 regularization)\n",
    "        gradient = A.T @ (A @ x - y) + lambda_val * x\n",
    "        \n",
    "        # For L2 regularization, we can directly update x using the gradient\n",
    "        # This is because the proximal operator for L2 regularization can be combined\n",
    "        # with the gradient step into a single update\n",
    "        x_next = x - alpha_k * gradient\n",
    "\n",
    "        # Check for convergence\n",
    "        if np.linalg.norm(x_next - x) < tol:\n",
    "            break\n",
    "\n",
    "        # Update x\n",
    "        x = x_next\n",
    "        history.append(x.copy())\n",
    "\n",
    "    return x, history\n",
    "\n",
    "# Load and prepare data\n",
    "data = pd.read_csv('./train_a.csv')\n",
    "\n",
    "A = data[['Value1','Value2']].values\n",
    "y = data['Result'].values\n",
    "\n",
    "# Add a column of ones to A for the intercept term\n",
    "A = np.hstack([A, np.ones((A.shape[0], 1))])\n",
    "\n",
    "# Set lambda value based on roll number\n",
    "R = 8  # last digit of roll number\n",
    "lambda_val = abs(R / 10 - 5)\n",
    "\n",
    "# Solve the optimization problem\n",
    "solution, history = proximal_gradient_method_l2(A, y, lambda_val)\n",
    "\n",
    "# Extract coefficients and intercept\n",
    "a = solution[:2]\n",
    "beta = solution[2]\n",
    "\n",
    "# Output the results\n",
    "print(\"Coefficients a:\", a)\n",
    "print(\"Intercept beta:\", beta)\n",
    "print(\"Solution is:\", solution)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 3\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### L1 Regularization\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Coefficients a: [-6.07354283e-02  9.54632440e+01  5.63261542e+01]\n",
      "Intercept beta: 23.18466936095755\n",
      "Solution is: [-6.07354283e-02  9.54632440e+01  5.63261542e+01  2.31846694e+01]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "def soft_thresholding(x, alpha):\n",
    "    return np.sign(x) * np.maximum(np.abs(x) - alpha, 0)\n",
    "\n",
    "def compute_step_size(A):\n",
    "    eigvals = np.linalg.eigvalsh(A.T @ A)\n",
    "    max_eigenvalue = np.max(eigvals)\n",
    "    # Step size alpha_k\n",
    "    alpha_k = 1 / (max_eigenvalue / 2 + 0.5)\n",
    "    return alpha_k\n",
    "\n",
    "def proximal_gradient_method(A, y, lambda_val, max_iterations=1000, tol=1e-6):\n",
    "    # Initialize x\n",
    "    x = np.zeros(A.shape[1])\n",
    "    history = [x.copy()]\n",
    "\n",
    "    # Compute the step size\n",
    "    alpha_k = compute_step_size(A)\n",
    "\n",
    "    for k in range(max_iterations):\n",
    "        # Gradient step\n",
    "        gradient = A.T @ (A @ x - y)\n",
    "        x_gradient_step = x - alpha_k * gradient\n",
    "\n",
    "        # Proximal step (soft-thresholding for L1 norm)\n",
    "        x_next = soft_thresholding(x_gradient_step, alpha_k * lambda_val)\n",
    "\n",
    "        # Check for convergence\n",
    "        if np.linalg.norm(x_next - x) < tol:\n",
    "            break\n",
    "\n",
    "        # Update x\n",
    "        x = x_next\n",
    "        history.append(x.copy())\n",
    "\n",
    "    return x, history\n",
    "\n",
    "\n",
    "data=pd.read_csv('./4 columns.csv')\n",
    "\n",
    "A = data[['area','bedrooms','bathrooms']].values\n",
    "y = data['price'].values\n",
    "\n",
    "# Add a column of ones to A for the intercept term\n",
    "A = np.hstack([A, np.ones((A.shape[0], 1))])\n",
    "\n",
    "\n",
    "R = 8  # last digit of roll number\n",
    "lambda_val = abs(R / 10 - 5)\n",
    "\n",
    "# Solve the optimization problem\n",
    "solution, history = proximal_gradient_method(A, y, lambda_val)\n",
    "\n",
    "# Extract coefficients and intercept\n",
    "a = solution[:3]\n",
    "beta = solution[3]\n",
    "\n",
    "# Output the results\n",
    "print(\"Coefficients a:\", a)\n",
    "print(\"Intercept beta:\", beta)\n",
    "print(\"Solution is:\", solution)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### L2 Regularization\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Coefficients [area, bedrooms, bathrooms]: [-6.30923131e-02  9.54631200e+01  5.63260822e+01]\n",
      "Intercept: 23.184641120164265\n",
      "Complete solution: [-6.30923131e-02  9.54631200e+01  5.63260822e+01  2.31846411e+01]\n",
      "\n",
      "Example prediction:\n",
      "Predicted price for house with 1500 sqft, 3 bedrooms, 2 bathrooms:\n",
      "$327.59\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "def compute_step_size(A):\n",
    "    eigvals = np.linalg.eigvalsh(A.T @ A)\n",
    "    max_eigenvalue = np.max(eigvals)\n",
    "    # Step size alpha_k\n",
    "    alpha_k = 1 / (max_eigenvalue / 2 + 0.5)\n",
    "    return alpha_k\n",
    "\n",
    "def proximal_gradient_method_l2(A, y, lambda_val, max_iterations=1000, tol=1e-6):\n",
    "    # Initialize x\n",
    "    x = np.zeros(A.shape[1])\n",
    "    history = [x.copy()]\n",
    "\n",
    "    # Compute the step size\n",
    "    alpha_k = compute_step_size(A)\n",
    "\n",
    "    for k in range(max_iterations):\n",
    "        # Gradient of the loss function (MSE + L2 regularization)\n",
    "        gradient = A.T @ (A @ x - y) + lambda_val * x\n",
    "        \n",
    "        # Update step for L2 regularization\n",
    "        # No need for proximal operator as L2 is smooth\n",
    "        x_next = x - alpha_k * gradient\n",
    "\n",
    "        # Check for convergence\n",
    "        if np.linalg.norm(x_next - x) < tol:\n",
    "            break\n",
    "\n",
    "        # Update x\n",
    "        x = x_next\n",
    "        history.append(x.copy())\n",
    "\n",
    "    return x, history\n",
    "\n",
    "# Load and prepare data\n",
    "data = pd.read_csv('./4 columns.csv')\n",
    "\n",
    "# Prepare feature matrix A and target vector y\n",
    "A = data[['area', 'bedrooms', 'bathrooms']].values\n",
    "y = data['price'].values\n",
    "\n",
    "# Add a column of ones to A for the intercept term\n",
    "A = np.hstack([A, np.ones((A.shape[0], 1))])\n",
    "\n",
    "# Set lambda value based on roll number\n",
    "R = 8  # last digit of roll number\n",
    "lambda_val = abs(R / 10 - 5)\n",
    "\n",
    "# Solve the optimization problem\n",
    "solution, history = proximal_gradient_method_l2(A, y, lambda_val)\n",
    "\n",
    "# Extract coefficients and intercept\n",
    "a = solution[:3]  # Coefficients for area, bedrooms, bathrooms\n",
    "beta = solution[3]  # Intercept term\n",
    "\n",
    "# Output the results\n",
    "print(\"Coefficients [area, bedrooms, bathrooms]:\", a)\n",
    "print(\"Intercept:\", beta)\n",
    "print(\"Complete solution:\", solution)\n",
    "\n",
    "# Optional: Add prediction functionality\n",
    "def predict_price(area, bedrooms, bathrooms):\n",
    "    features = np.array([area, bedrooms, bathrooms, 1])\n",
    "    return np.dot(solution, features)\n",
    "\n",
    "# Example prediction\n",
    "print(\"\\nExample prediction:\")\n",
    "example_house = [1500, 3, 2]  # area, bedrooms, bathrooms\n",
    "predicted_price = predict_price(*example_house)\n",
    "print(f\"Predicted price for house with {example_house[0]} sqft, {example_house[1]} bedrooms, {example_house[2]} bathrooms:\")\n",
    "print(f\"${predicted_price:,.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 4\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### L1 Regularization\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimal parameters (θ₁, θ₂, θ₃): [0. 0. 0.]\n",
      "\n",
      "Estimated purchase frequency for loyalty score 10.80: 1.0\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "def model_function(x, theta):\n",
    "    \"\"\"\n",
    "    Computes y = exp(theta[0]*x) * (cos(theta[1]*x) + sin(theta[2]*x))\n",
    "    \"\"\"\n",
    "    return np.exp(theta[0]*x) * (np.cos(theta[1]*x) + np.sin(theta[2]*x))\n",
    "\n",
    "def compute_gradient(x, y, theta):\n",
    "    \"\"\"\n",
    "    Computes the gradient of the loss function with respect to theta\n",
    "    \"\"\"\n",
    "    m = len(x)\n",
    "    y_pred = model_function(x, theta)\n",
    "    error = y_pred - y\n",
    "    \n",
    "    # Partial derivatives\n",
    "    grad = np.zeros(3)\n",
    "    \n",
    "    # ∂/∂θ₁\n",
    "    grad[0] = np.sum(error * x * y_pred)\n",
    "    \n",
    "    # ∂/∂θ₂\n",
    "    grad[1] = np.sum(error * np.exp(theta[0]*x) * (-x*np.sin(theta[1]*x)))\n",
    "    \n",
    "    # ∂/∂θ₃\n",
    "    grad[2] = np.sum(error * np.exp(theta[0]*x) * (x*np.cos(theta[2]*x)))\n",
    "    \n",
    "    return grad / m\n",
    "\n",
    "def soft_thresholding(x, alpha):\n",
    "    \"\"\"\n",
    "    Soft thresholding operator for L1 regularization\n",
    "    \"\"\"\n",
    "    return np.sign(x) * np.maximum(np.abs(x) - alpha, 0)\n",
    "\n",
    "def proximal_gradient_method(x, y, lambda_val, max_iterations=1000, learning_rate=0.01, tol=1e-6):\n",
    "    # Initialize parameters\n",
    "    theta = np.zeros(3)\n",
    "    history = [theta.copy()]\n",
    "    \n",
    "    for iteration in range(max_iterations):\n",
    "        # Store previous theta for convergence check\n",
    "        theta_prev = theta.copy()\n",
    "        \n",
    "        # Gradient step\n",
    "        gradient = compute_gradient(x, y, theta)\n",
    "        theta_gradient_step = theta - learning_rate * gradient\n",
    "        \n",
    "        # Proximal step (L1 regularization)\n",
    "        theta = soft_thresholding(theta_gradient_step, learning_rate * lambda_val)\n",
    "        \n",
    "        # Check convergence\n",
    "        if np.linalg.norm(theta - theta_prev) < tol:\n",
    "            break\n",
    "            \n",
    "        history.append(theta.copy())\n",
    "    \n",
    "    return theta, history\n",
    "\n",
    "# Load data\n",
    "data = pd.read_csv('./Customer Purchasing Behaviors (1).csv')\n",
    "x = data['loyalty_score'].values\n",
    "y = data['purchase_frequency'].values\n",
    "\n",
    "# Set lambda value based on roll number (R/10)\n",
    "R = 98  # Replace with your last two digits of roll number\n",
    "lambda_val = R/10 + 1\n",
    "\n",
    "# Normalize the input features for better convergence\n",
    "x = (x - x.mean()) / x.std()\n",
    "\n",
    "# Find optimal parameters\n",
    "theta_optimal, history = proximal_gradient_method(x, y, lambda_val)\n",
    "\n",
    "print(\"Optimal parameters (θ₁, θ₂, θ₃):\", theta_optimal)\n",
    "\n",
    "# Function to estimate purchase frequency for a given loyalty score\n",
    "def estimate_purchase_frequency(loyalty_score):\n",
    "    # Normalize the input using the same parameters as training data\n",
    "    x_normalized = (loyalty_score - x.mean()) / x.std()\n",
    "    return model_function(x_normalized, theta_optimal)\n",
    "\n",
    "# Example prediction for R/10 + 1\n",
    "test_value = R/10 + 1\n",
    "predicted_frequency = estimate_purchase_frequency(test_value)\n",
    "print(f\"\\nEstimated purchase frequency for loyalty score {test_value:.2f}:\", predicted_frequency)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### L2 Regularization\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "omllabs",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
