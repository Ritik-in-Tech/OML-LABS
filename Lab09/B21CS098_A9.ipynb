{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 1\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### L1 Regularization\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Solution with fixed step size: [1.49999928 1.49999928]\n",
      "Solution with variable step size: [1.40186916 1.40186916]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "def soft_thresholding(x, alpha):\n",
    "    return np.sign(x) * np.maximum(np.abs(x) - alpha, 0)\n",
    "\n",
    "def proximal_gradient_method(initial_x, max_iterations=1000, alpha_type=\"fixed\", r=1, tol=1e-6):\n",
    "    x = np.array(initial_x, dtype=float)\n",
    "    history = [x.copy()]\n",
    "    \n",
    "    for k in range(max_iterations):\n",
    "        if alpha_type == \"fixed\":\n",
    "            alpha_k = 1/2\n",
    "        elif alpha_type == \"variable\":\n",
    "            alpha_k = 1 / (r + k)\n",
    "        else:\n",
    "            raise ValueError(\"alpha_type must be 'fixed' or 'variable'\")\n",
    "        \n",
    "        # Gradient of the smooth part\n",
    "        gradient_g = np.array([x[0] - 2, x[1] - 2])\n",
    "        \n",
    "        # Gradient step\n",
    "        x_gradient_step = x - alpha_k * gradient_g\n",
    "        \n",
    "        # Proximal step (using soft-thresholding for L1 norm)\n",
    "        x_next = soft_thresholding(x_gradient_step, alpha_k * 0.5)\n",
    "        \n",
    "        # Check for convergence\n",
    "        if np.linalg.norm(x_next - x) < tol:\n",
    "            break\n",
    "        \n",
    "        # Update x\n",
    "        x = x_next\n",
    "        history.append(x.copy())\n",
    "    \n",
    "    return x, history\n",
    "\n",
    "# Parameters\n",
    "initial_x = [0.0, 0.0]  # Initial guess\n",
    "r = 8  # Example last digit of roll number\n",
    "\n",
    "# Solve with fixed step size\n",
    "solution_fixed, history_fixed = proximal_gradient_method(initial_x, max_iterations=100, alpha_type=\"fixed\", r=r)\n",
    "\n",
    "# Solve with variable step size\n",
    "solution_variable, history_variable = proximal_gradient_method(initial_x, max_iterations=100, alpha_type=\"variable\", r=r)\n",
    "\n",
    "# Results\n",
    "print(\"Solution with fixed step size:\", solution_fixed)\n",
    "print(\"Solution with variable step size:\", solution_variable)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### L2 Regularization\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Solution with fixed step size: [1.33333276 1.33333276]\n",
      "Solution with variable step size: [1.30993379 1.30993379]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "def l2_proximal_operator(x, alpha):\n",
    "    \"\"\"\n",
    "    Proximal operator for L2 regularization (ridge)\n",
    "    For L2 norm, the proximal operator is: x / (1 + alpha)\n",
    "    \"\"\"\n",
    "    return x / (1 + alpha)\n",
    "\n",
    "def proximal_gradient_method_l2(initial_x, max_iterations=1000, alpha_type=\"fixed\", r=1, tol=1e-6):\n",
    "    x = np.array(initial_x, dtype=float)\n",
    "    history = [x.copy()]\n",
    "    \n",
    "    for k in range(max_iterations):\n",
    "        if alpha_type == \"fixed\":\n",
    "            alpha_k = 1/2\n",
    "        elif alpha_type == \"variable\":\n",
    "            alpha_k = 1 / (r + k)\n",
    "        else:\n",
    "            raise ValueError(\"alpha_type must be 'fixed' or 'variable'\")\n",
    "        \n",
    "        # Gradient of the smooth part (quadratic loss)\n",
    "        gradient_g = np.array([x[0] - 2, x[1] - 2])\n",
    "        \n",
    "        # Gradient step\n",
    "        x_gradient_step = x - alpha_k * gradient_g\n",
    "        \n",
    "        # Proximal step (using L2 proximal operator)\n",
    "        # The 0.5 factor is the regularization parameter (similar to your original code)\n",
    "        x_next = l2_proximal_operator(x_gradient_step, alpha_k * 0.5)\n",
    "        \n",
    "        # Check for convergence\n",
    "        if np.linalg.norm(x_next - x) < tol:\n",
    "            break\n",
    "        \n",
    "        # Update x\n",
    "        x = x_next\n",
    "        history.append(x.copy())\n",
    "    \n",
    "    return x, history\n",
    "\n",
    "# Parameters\n",
    "initial_x = [0.0, 0.0]  # Initial guess\n",
    "r = 8  # Example last digit of roll number\n",
    "\n",
    "# Solve with fixed step size\n",
    "solution_fixed, history_fixed = proximal_gradient_method_l2(initial_x, max_iterations=100, alpha_type=\"fixed\", r=r)\n",
    "\n",
    "# Solve with variable step size\n",
    "solution_variable, history_variable = proximal_gradient_method_l2(initial_x, max_iterations=100, alpha_type=\"variable\", r=r)\n",
    "\n",
    "# Results\n",
    "print(\"Solution with fixed step size:\", solution_fixed)\n",
    "print(\"Solution with variable step size:\", solution_variable)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 2\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### L1 Regularization\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Coefficients a: [-0.20587412 -0.45864919]\n",
      "Intercept beta: 4.7441788010967956e-07\n",
      "Solution is: [-2.05874121e-01 -4.58649186e-01  4.74417880e-07]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "def soft_thresholding(x, alpha):\n",
    "    return np.sign(x) * np.maximum(np.abs(x) - alpha, 0)\n",
    "\n",
    "def compute_step_size(A):\n",
    "    eigvals = np.linalg.eigvalsh(A.T @ A)\n",
    "    max_eigenvalue = np.max(eigvals)\n",
    "    # Step size alpha_k\n",
    "    alpha_k = 1 / (max_eigenvalue / 2 + 0.5)\n",
    "    return alpha_k\n",
    "\n",
    "def proximal_gradient_method(A, y, lambda_val, max_iterations=1000, tol=1e-6):\n",
    "    # Initialize x\n",
    "    x = np.zeros(A.shape[1])\n",
    "    history = [x.copy()]\n",
    "\n",
    "    # Compute the step size\n",
    "    alpha_k = compute_step_size(A)\n",
    "\n",
    "    for k in range(max_iterations):\n",
    "        # Gradient step\n",
    "        gradient = A.T @ (A @ x - y)\n",
    "        x_gradient_step = x - alpha_k * gradient\n",
    "\n",
    "        # Proximal step (soft-thresholding for L1 norm)\n",
    "        x_next = soft_thresholding(x_gradient_step, alpha_k * lambda_val)\n",
    "\n",
    "        # Check for convergence\n",
    "        if np.linalg.norm(x_next - x) < tol:\n",
    "            break\n",
    "\n",
    "        # Update x\n",
    "        x = x_next\n",
    "        history.append(x.copy())\n",
    "\n",
    "    return x, history\n",
    "\n",
    "\n",
    "data=pd.read_csv('./train_a.csv')\n",
    "\n",
    "A = data[['Value1','Value2']].values\n",
    "y = data['Result'].values\n",
    "\n",
    "# Add a column of ones to A for the intercept term\n",
    "A = np.hstack([A, np.ones((A.shape[0], 1))])\n",
    "\n",
    "\n",
    "R = 8  # last digit of roll number\n",
    "lambda_val = abs(R / 10 - 5)\n",
    "\n",
    "# Solve the optimization problem\n",
    "solution, history = proximal_gradient_method(A, y, lambda_val)\n",
    "\n",
    "# Extract coefficients and intercept\n",
    "a = solution[:2]\n",
    "beta = solution[2]\n",
    "\n",
    "# Output the results\n",
    "print(\"Coefficients a:\", a)\n",
    "print(\"Intercept beta:\", beta)\n",
    "print(\"Solution is:\", solution)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### L2 Regularization\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Coefficients a: [-0.20591411 -0.45863131]\n",
      "Intercept beta: -4.741042437662977e-06\n",
      "Solution is: [-2.05914109e-01 -4.58631309e-01 -4.74104244e-06]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "def compute_step_size(A):\n",
    "    eigvals = np.linalg.eigvalsh(A.T @ A)\n",
    "    max_eigenvalue = np.max(eigvals)\n",
    "    # Step size alpha_k\n",
    "    alpha_k = 1 / (max_eigenvalue / 2 + 0.5)\n",
    "    return alpha_k\n",
    "\n",
    "def proximal_gradient_method_l2(A, y, lambda_val, max_iterations=1000, tol=1e-6):\n",
    "    # Initialize x\n",
    "    x = np.zeros(A.shape[1])\n",
    "    history = [x.copy()]\n",
    "\n",
    "    # Compute the step size\n",
    "    alpha_k = compute_step_size(A)\n",
    "\n",
    "    for k in range(max_iterations):\n",
    "        # Gradient of the smooth part (least squares loss + L2 regularization)\n",
    "        gradient = A.T @ (A @ x - y) + lambda_val * x\n",
    "        x_next = x - alpha_k * gradient\n",
    "\n",
    "        # Check for convergence\n",
    "        if np.linalg.norm(x_next - x) < tol:\n",
    "            break\n",
    "\n",
    "        # Update x\n",
    "        x = x_next\n",
    "        history.append(x.copy())\n",
    "\n",
    "    return x, history\n",
    "\n",
    "# Load and prepare data\n",
    "data = pd.read_csv('./train_a.csv')\n",
    "\n",
    "A = data[['Value1','Value2']].values\n",
    "y = data['Result'].values\n",
    "\n",
    "# Add a column of ones to A for the intercept term\n",
    "A = np.hstack([A, np.ones((A.shape[0], 1))])\n",
    "\n",
    "# Set lambda value based on roll number\n",
    "R = 8  # last digit of roll number\n",
    "lambda_val = abs(R / 10 - 5)\n",
    "\n",
    "# Solve the optimization problem\n",
    "solution, history = proximal_gradient_method_l2(A, y, lambda_val)\n",
    "\n",
    "# Extract coefficients and intercept\n",
    "a = solution[:2]\n",
    "beta = solution[2]\n",
    "\n",
    "# Output the results\n",
    "print(\"Coefficients a:\", a)\n",
    "print(\"Intercept beta:\", beta)\n",
    "print(\"Solution is:\", solution)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 3\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### L1 Regularization\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Coefficients [area, bedrooms, bathrooms]: [-6.07354283e-02  9.54632440e+01  5.63261542e+01]\n",
      "Intercept beta: 23.18466936095755\n",
      "Solution is: [-6.07354283e-02  9.54632440e+01  5.63261542e+01  2.31846694e+01]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "def soft_thresholding(x, alpha):\n",
    "    return np.sign(x) * np.maximum(np.abs(x) - alpha, 0)\n",
    "\n",
    "def compute_step_size(A):\n",
    "    eigvals = np.linalg.eigvalsh(A.T @ A)\n",
    "    max_eigenvalue = np.max(eigvals)\n",
    "    alpha_k = 1 / (max_eigenvalue / 2 + 0.5)\n",
    "    return alpha_k\n",
    "\n",
    "def proximal_gradient_method(A, y, lambda_val, max_iterations=1000, tol=1e-6):\n",
    "    # Initialize x\n",
    "    x = np.zeros(A.shape[1])\n",
    "    history = [x.copy()]\n",
    "\n",
    "    # Compute the step size\n",
    "    alpha_k = compute_step_size(A)\n",
    "\n",
    "    for k in range(max_iterations):\n",
    "        # Gradient step\n",
    "        gradient = A.T @ (A @ x - y)\n",
    "        x_gradient_step = x - alpha_k * gradient\n",
    "\n",
    "        # Proximal step (soft-thresholding for L1 norm)\n",
    "        x_next = soft_thresholding(x_gradient_step, alpha_k * lambda_val)\n",
    "\n",
    "        # Check for convergence\n",
    "        if np.linalg.norm(x_next - x) < tol:\n",
    "            break\n",
    "\n",
    "        # Update x\n",
    "        x = x_next\n",
    "        history.append(x.copy())\n",
    "\n",
    "    return x, history\n",
    "\n",
    "\n",
    "data=pd.read_csv('./4 columns.csv')\n",
    "\n",
    "A = data[['area','bedrooms','bathrooms']].values\n",
    "y = data['price'].values\n",
    "\n",
    "# Add a column of ones to A for the intercept term\n",
    "A = np.hstack([A, np.ones((A.shape[0], 1))])\n",
    "\n",
    "\n",
    "R = 8  # last digit of roll number\n",
    "lambda_val = abs(R / 10 - 5)\n",
    "\n",
    "# Solve the optimization problem\n",
    "solution, history = proximal_gradient_method(A, y, lambda_val)\n",
    "\n",
    "# Extract coefficients and intercept\n",
    "a = solution[:3]\n",
    "beta = solution[3]\n",
    "\n",
    "# Output the results\n",
    "print(\"Coefficients [area, bedrooms, bathrooms]:\", a)\n",
    "print(\"Intercept beta:\", beta)\n",
    "print(\"Solution is:\", solution)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### L2 Regularization\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Coefficients [area, bedrooms, bathrooms]: [-6.30923131e-02  9.54631200e+01  5.63260822e+01]\n",
      "Intercept: 23.184641120164265\n",
      "Complete solution: [-6.30923131e-02  9.54631200e+01  5.63260822e+01  2.31846411e+01]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "def compute_step_size(A):\n",
    "    eigvals = np.linalg.eigvalsh(A.T @ A)\n",
    "    max_eigenvalue = np.max(eigvals)\n",
    "    alpha_k = 1 / (max_eigenvalue / 2 + 0.5)\n",
    "    return alpha_k\n",
    "\n",
    "def proximal_gradient_method_l2(A, y, lambda_val, max_iterations=1000, tol=1e-6):\n",
    "    # Initialize x\n",
    "    x = np.zeros(A.shape[1])\n",
    "    history = [x.copy()]\n",
    "\n",
    "    # Compute the step size\n",
    "    alpha_k = compute_step_size(A)\n",
    "\n",
    "    for k in range(max_iterations):\n",
    "        # Gradient of the loss function (MSE + L2 regularization)\n",
    "        gradient = A.T @ (A @ x - y) + lambda_val * x\n",
    "        \n",
    "        \n",
    "        x_next = x - alpha_k * gradient\n",
    "\n",
    "        # Check for convergence\n",
    "        if np.linalg.norm(x_next - x) < tol:\n",
    "            break\n",
    "\n",
    "        # Update x\n",
    "        x = x_next\n",
    "        history.append(x.copy())\n",
    "\n",
    "    return x, history\n",
    "\n",
    "# Load and prepare data\n",
    "data = pd.read_csv('./4 columns.csv')\n",
    "\n",
    "# Prepare feature matrix A and target vector y\n",
    "A = data[['area', 'bedrooms', 'bathrooms']].values\n",
    "y = data['price'].values\n",
    "\n",
    "\n",
    "A = np.hstack([A, np.ones((A.shape[0], 1))])\n",
    "\n",
    "R = 8   \n",
    "lambda_val = abs(R / 10 - 5)\n",
    "\n",
    "solution, history = proximal_gradient_method_l2(A, y, lambda_val)\n",
    "\n",
    "a = solution[:3]  \n",
    "beta = solution[3]   \n",
    "\n",
    "# Output the results\n",
    "print(\"Coefficients [area, bedrooms, bathrooms]:\", a)\n",
    "print(\"Intercept:\", beta)\n",
    "print(\"Complete solution:\", solution)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 4\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### L1 Regularization\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimal θ values:\n",
      "θ1: 0.37\n",
      "θ2: -0.00\n",
      "θ3: 0.37\n",
      "λ: 4.80\n",
      "Estimated purchase frequency for loyalty score 10.80: 15.86\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy.optimize import minimize\n",
    "\n",
    "data = pd.read_csv('./Customer Purchasing Behaviors (1).csv')\n",
    "\n",
    "x = data['loyalty_score'].values\n",
    "y = data['purchase_frequency'].values\n",
    "\n",
    "R = 98\n",
    "target_x = R / 10 + 1\n",
    "\n",
    "lambda_value = abs(R / 10 - 5)\n",
    "\n",
    "def model(theta, x):\n",
    "    return np.exp(theta[0] * x) * (np.cos(theta[1] * x) + np.sin(theta[2] * x))\n",
    "\n",
    "def objective(theta):\n",
    "    residual = model(theta, x) - y\n",
    "    return 0.5 * np.sum(residual**2) + (lambda_value / 2) * np.sum(np.abs(theta))\n",
    "\n",
    "theta0 = np.array([0.1, 0.1, 0.1])\n",
    "\n",
    "result = minimize(objective, theta0, method='L-BFGS-B')\n",
    "\n",
    "optimal_theta = result.x\n",
    "print(\"Optimal θ values:\")\n",
    "print(f\"θ1: {optimal_theta[0]:.2f}\")\n",
    "print(f\"θ2: {optimal_theta[1]:.2f}\")\n",
    "print(f\"θ3: {optimal_theta[2]:.2f}\")\n",
    "print(f\"λ: {lambda_value:.2f}\")\n",
    "\n",
    "predicted_purchase_frequency = model(optimal_theta, target_x)\n",
    "print(f\"Estimated purchase frequency for loyalty score {target_x:.2f}: {predicted_purchase_frequency:.2f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### L2 Regularization\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Optimization Results:\n",
      "Optimal θ values:\n",
      "θ1: 0.3747\n",
      "θ2: 0.0000\n",
      "θ3: 0.3657\n",
      "λ: 4.8000\n",
      "Estimated purchase frequency for loyality score 10.8: 15.8587\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Load data\n",
    "data = pd.read_csv('./Customer Purchasing Behaviors (1).csv')\n",
    "\n",
    "x = data['loyalty_score'].values\n",
    "y = data['purchase_frequency'].values\n",
    "\n",
    "# Parameters based on roll number\n",
    "R = 98\n",
    "target_x = R / 10 + 1\n",
    "lambda_value = abs(R / 10 - 5)\n",
    "\n",
    "def model(theta, x):\n",
    "    return np.exp(theta[0] * x) * (np.cos(theta[1] * x) + np.sin(theta[2] * x))\n",
    "\n",
    "def objective(theta):\n",
    "    residual = model(theta, x) - y\n",
    "    l2_penalty = (lambda_value / 2) * np.sum(theta**2)\n",
    "    return 0.5 * np.sum(residual**2) + l2_penalty\n",
    "\n",
    "# Initial guess for parameters\n",
    "theta0 = np.array([0.1, 0.1, 0.1])\n",
    "\n",
    "# Optimize using L-BFGS-B method\n",
    "result = minimize(objective, theta0, method='L-BFGS-B')\n",
    "\n",
    "# Extract optimal parameters\n",
    "optimal_theta = result.x\n",
    "\n",
    "# Print results\n",
    "print(\"\\nOptimization Results:\")\n",
    "print(\"Optimal θ values:\")\n",
    "print(f\"θ1: {optimal_theta[0]:.4f}\")\n",
    "print(f\"θ2: {optimal_theta[1]:.4f}\")\n",
    "print(f\"θ3: {optimal_theta[2]:.4f}\")\n",
    "print(f\"λ: {lambda_value:.4f}\")\n",
    "\n",
    "\n",
    "predicted_purchase_frequency = model(optimal_theta, target_x)\n",
    "print(f\"Estimated purchase frequency for loyality score {target_x}: {predicted_purchase_frequency:.4f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "omllabs",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
