{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question01\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hessian matrix is not positive definite\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "def f(x): # Objective function\n",
    "    x1, x2 = x\n",
    "    return (1 - x1)**2 + (x2 - x1**2)**2\n",
    "\n",
    "def grad_f(x): # Gradient of the objective function\n",
    "    x1, x2 = x\n",
    "    df_dx1 = -2 * (1 - x1) - 4 * x1 * (x2 - x1**2)\n",
    "    df_dx2 = 2 * (x2 - x1**2)\n",
    "    return np.array([df_dx1, df_dx2])\n",
    "\n",
    "def hessian_f(x): # Hessian of the objective function\n",
    "    x1, x2 = x\n",
    "    d2f_dx1x1 = 2 - 4 * (x2 - x1**2) + 8 * x1**2\n",
    "    d2f_dx1x2 = -4 * x1\n",
    "    d2f_dx2x2 = 2\n",
    "    return np.array([[d2f_dx1x1, d2f_dx1x2], [d2f_dx1x2, d2f_dx2x2]])\n",
    "\n",
    "def backtracking_line_search(f, grad_f, x, p, alpha=0.3, beta=0.8): # Inexact line search using backtracking\n",
    "    t = 1\n",
    "    while f(x + t * p) > f(x) + alpha * t * np.dot(grad_f(x), p):\n",
    "        t *= beta\n",
    "    return t\n",
    "\n",
    "def modified_newton_method(f, grad_f, hessian_f, x0, tol=1e-4, max_iter=2000): # Modified Newton's method\n",
    "    x = np.array(x0)\n",
    "    for i in range(max_iter):\n",
    "        grad = grad_f(x)\n",
    "        hess = hessian_f(x)\n",
    "\n",
    "        # Check the stopping criterion\n",
    "        grad_norm = np.linalg.norm(grad)\n",
    "        if grad_norm < tol:\n",
    "            print(f'Converged in {i} iterations.')\n",
    "            return x, f(x)\n",
    "\n",
    "        # Check if Hessian is positive definite using Cholesky decomposition\n",
    "        try:\n",
    "            np.linalg.cholesky(hess)\n",
    "        except np.linalg.LinAlgError:\n",
    "            return \"Hessian matrix is not positive definite\"\n",
    "        \n",
    "        # Compute the Newton direction\n",
    "        p = -np.linalg.solve(hess, grad)\n",
    "        \n",
    "        # Perform backtracking line search\n",
    "        t = backtracking_line_search(f, grad_f, x, p)\n",
    "        \n",
    "        # Update the solution\n",
    "        x = x + t * p\n",
    "    \n",
    "    print('Maximum iterations reached.')\n",
    "    return x, f(x)\n",
    "\n",
    "x0 = [0, 3] # Initial approximation\n",
    "\n",
    " \n",
    "result = modified_newton_method(f, grad_f, hessian_f, x0) # Run the modified Newton method\n",
    "\n",
    "if isinstance(result, str):\n",
    "    print(result)\n",
    "else:\n",
    "    solution, f_value = result\n",
    "    print(f\"Solution: {solution}\")\n",
    "    print(f\"Function value at the solution: {f_value}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question02\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converged in 5 iterations.\n",
      "Optimized weights: [-8.40469636e+00  1.23182298e-01  3.51637146e-02 -1.32955469e-02\n",
      "  6.18964365e-04 -1.19169898e-03  8.97009700e-02  9.45179740e-01\n",
      "  1.48690047e-02]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy.special import expit   \n",
    "from numpy.linalg import LinAlgError\n",
    "\n",
    "data =pd.read_csv('../Lab06/diabetes2.csv') # Load the dataset\n",
    "\n",
    "# print(data.head)\n",
    "\n",
    "X = data.drop(columns='Outcome').values   # Extract features and labels\n",
    "y = data['Outcome'].values  # Labels\n",
    "y = 2 * y - 1  # Convert to {-1, 1}\n",
    "\n",
    "X = np.hstack([np.ones((X.shape[0], 1)), X]) # Add an intercept term to X\n",
    "\n",
    "# Sigmoid function\n",
    "def sigmoid(z):\n",
    "    return expit(z)\n",
    "\n",
    "# Logistic loss function\n",
    "def logistic_loss(x, X, y):\n",
    "    z = X @ x\n",
    "    return -np.sum(np.log(sigmoid(y * z)))\n",
    "\n",
    "# Gradient of the logistic loss\n",
    "def grad_logistic_loss(x, X, y):\n",
    "    z = X @ x\n",
    "    grad = -X.T @ (y * (1 - sigmoid(y * z)))\n",
    "    return grad\n",
    "\n",
    "# Hessian of the logistic loss\n",
    "def hessian_logistic_loss(x, X, y):\n",
    "    z = X @ x\n",
    "    S = sigmoid(y * z) * (1 - sigmoid(y * z))\n",
    "    H = X.T @ np.diag(S) @ X\n",
    "    return H\n",
    "\n",
    "# Modified Newton's method with logistic regression\n",
    "def modified_newton_logistic(X, y, tol=1e-4, max_iter=2000):\n",
    "    x = np.zeros(X.shape[1])  # Initialize weights\n",
    "    for i in range(max_iter):\n",
    "        grad = grad_logistic_loss(x, X, y)\n",
    "        hess = hessian_logistic_loss(x, X, y)\n",
    "\n",
    "        # Check if the gradient norm is small enough (stopping criterion)\n",
    "        if np.linalg.norm(grad) < tol:\n",
    "            print(f'Converged in {i} iterations.')\n",
    "            return x\n",
    "        \n",
    "        # Check if the Hessian is positive definite\n",
    "        try:\n",
    "            np.linalg.cholesky(hess)\n",
    "        except LinAlgError:\n",
    "            print(\"Hessian matrix is not positive definite.\")\n",
    "            return None\n",
    "        \n",
    "        # Update the parameter vector x using Newton's method\n",
    "        delta_x = np.linalg.solve(hess, grad)\n",
    "        x = x - delta_x\n",
    "\n",
    "    print(\"Maximum iterations reached.\")\n",
    "    return x\n",
    "\n",
    "# Apply the modified Newton method\n",
    "solution = modified_newton_logistic(X, y)\n",
    "\n",
    "if solution is not None:\n",
    "    print(f\"Optimized weights: {solution}\")\n",
    "else:\n",
    "    print(\"Optimization failed.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question03\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hessian matrix is not positive definite.\n",
      "Optimization failed.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from numpy.linalg import LinAlgError\n",
    "\n",
    "# Sample dataset\n",
    "data = pd.read_csv('./Customer Purchasing Behaviors.csv')\n",
    "\n",
    "# Extract loyalty score (x) and purchase frequency (y)\n",
    "x = data['loyalty_score'].values\n",
    "y = data['purchase_frequency'].values\n",
    "\n",
    "# Define the model for purchase frequency\n",
    "def model(theta, x):\n",
    "    theta1, theta2, theta3 = theta\n",
    "    return np.exp(theta1 * x) * (np.cos(theta2 * x) + np.sin(theta3 * x))\n",
    "\n",
    "# Loss function (squared error)\n",
    "def loss_function(theta, x, y):\n",
    "    y_pred = model(theta, x)\n",
    "    return np.sum((y - y_pred) ** 2)\n",
    "\n",
    "# Gradient of the loss function\n",
    "def grad_loss_function(theta, x, y):\n",
    "    theta1, theta2, theta3 = theta\n",
    "    y_pred = model(theta, x)\n",
    "    error = y - y_pred\n",
    "    \n",
    "    # Partial derivatives\n",
    "    dtheta1 = -2 * np.sum(error * (x * np.exp(theta1 * x)) * (np.cos(theta2 * x) + np.sin(theta3 * x)))\n",
    "    dtheta2 = -2 * np.sum(error * np.exp(theta1 * x) * (-x * np.sin(theta2 * x)))\n",
    "    dtheta3 = -2 * np.sum(error * np.exp(theta1 * x) * (x * np.cos(theta3 * x)))\n",
    "    \n",
    "    return np.array([dtheta1, dtheta2, dtheta3])\n",
    "\n",
    "# Hessian of the loss function\n",
    "def hessian_loss_function(theta, x, y):\n",
    "    theta1, theta2, theta3 = theta\n",
    "    y_pred = model(theta, x)\n",
    "    error = y - y_pred\n",
    "    \n",
    "    # Second order partial derivatives\n",
    "    d2theta1 = 2 * np.sum((x ** 2) * np.exp(2 * theta1 * x) * (np.cos(theta2 * x) + np.sin(theta3 * x)) ** 2)\n",
    "    d2theta2 = 2 * np.sum(error * np.exp(theta1 * x) * (x ** 2) * np.cos(theta2 * x))\n",
    "    d2theta3 = 2 * np.sum(error * np.exp(theta1 * x) * (x ** 2) * -np.sin(theta3 * x))\n",
    "    \n",
    "    # Cross partial derivatives\n",
    "    dtheta1theta2 = 2 * np.sum(error * x * np.exp(theta1 * x) * (-x * np.sin(theta2 * x)))\n",
    "    dtheta1theta3 = 2 * np.sum(error * x * np.exp(theta1 * x) * (x * np.cos(theta3 * x)))\n",
    "    dtheta2theta3 = 2 * np.sum(error * np.exp(theta1 * x) * (-x ** 2 * np.sin(theta2 * x) * np.cos(theta3 * x)))\n",
    "    \n",
    "    # Hessian matrix\n",
    "    hess = np.array([\n",
    "        [d2theta1, dtheta1theta2, dtheta1theta3],\n",
    "        [dtheta1theta2, d2theta2, dtheta2theta3],\n",
    "        [dtheta1theta3, dtheta2theta3, d2theta3]\n",
    "    ])\n",
    "    \n",
    "    return hess\n",
    "\n",
    "# Modified Newton's method\n",
    "def modified_newton(x, y, tol=1e-4, max_iter=100):\n",
    "    theta = np.zeros(3)  # Initialize theta1, theta2, theta3\n",
    "    \n",
    "    for i in range(max_iter):\n",
    "        grad = grad_loss_function(theta, x, y)\n",
    "        hess = hessian_loss_function(theta, x, y)\n",
    "        \n",
    "        # Check if the gradient norm is small enough (stopping criterion)\n",
    "        if np.linalg.norm(grad) < tol:\n",
    "            print(f'Converged in {i} iterations.')\n",
    "            return theta\n",
    "        \n",
    "        # Check if the Hessian is positive definite\n",
    "        try:\n",
    "            np.linalg.cholesky(hess)\n",
    "        except LinAlgError:\n",
    "            print(\"Hessian matrix is not positive definite.\")\n",
    "            return None\n",
    "        \n",
    "        # Update the parameters using Newton's method\n",
    "        delta_theta = np.linalg.solve(hess, grad)\n",
    "        theta = theta - delta_theta\n",
    "    \n",
    "    print(\"Maximum iterations reached.\")\n",
    "    return theta\n",
    "\n",
    "# Apply the modified Newton method\n",
    "theta_optimal = modified_newton(x, y)\n",
    "\n",
    "# If optimization succeeded, estimate purchase frequency for R/10 + 1 where R = 98\n",
    "if theta_optimal is not None:\n",
    "    R = 98\n",
    "    x_new = R / 10 + 1\n",
    "    purchase_frequency_estimate = model(theta_optimal, x_new)\n",
    "    print(f\"Estimated purchase frequency for loyalty score {x_new}: {purchase_frequency_estimate}\")\n",
    "else:\n",
    "    print(\"Optimization failed.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question04\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Singular matrix encountered at iteration 0.\n",
      "Optimization failed.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "def load_data_from_csv(file_path): # Load the CSV file using pandas\n",
    "    data = pd.read_csv(file_path)\n",
    "    x1 = data['x1'].values\n",
    "    x2 = data['x2'].values\n",
    "    y = data['Y'].values\n",
    "    return x1, x2, y\n",
    "\n",
    "def model(beta, x1, x2): # Define the model for prediction\n",
    "    beta1, beta2 = beta\n",
    "    exp_term = np.exp(beta1 * x1 + beta2 * x2)\n",
    "    return exp_term / (1 + exp_term)\n",
    "\n",
    "\n",
    "def loss_function(beta, x1, x2, y): # Loss function (squared error)\n",
    "    y_pred = model(beta, x1, x2)\n",
    "    return np.sum((y - y_pred) ** 2)\n",
    "\n",
    " \n",
    "def grad_loss_function(beta, x1, x2, y): # Gradient of the loss function\n",
    "    beta1, beta2 = beta\n",
    "    y_pred = model(beta, x1, x2)\n",
    "    error = y - y_pred\n",
    "    \n",
    "    # Partial derivatives\n",
    "    d_beta1 = -2 * np.sum(error * (x1 * y_pred * (1 - y_pred)))\n",
    "    d_beta2 = -2 * np.sum(error * (x2 * y_pred * (1 - y_pred)))\n",
    "    \n",
    "    return np.array([d_beta1, d_beta2])\n",
    "\n",
    "# Hessian of the loss function\n",
    "def hessian_loss_function(beta, x1, x2, y):\n",
    "    beta1, beta2 = beta\n",
    "    y_pred = model(beta, x1, x2)\n",
    "    error = y - y_pred\n",
    "    \n",
    "    # Second order partial derivatives\n",
    "    d2_beta1 = 2 * np.sum((x1 ** 2) * y_pred * (1 - y_pred) * (1 - 2 * y_pred))\n",
    "    d2_beta2 = 2 * np.sum((x2 ** 2) * y_pred * (1 - y_pred) * (1 - 2 * y_pred))\n",
    "    \n",
    "    # Cross partial derivatives\n",
    "    d_beta1_beta2 = 2 * np.sum(x1 * x2 * y_pred * (1 - y_pred) * (1 - 2 * y_pred))\n",
    "    \n",
    "    # Hessian matrix\n",
    "    hess = np.array([\n",
    "        [d2_beta1, d_beta1_beta2],\n",
    "        [d_beta1_beta2, d2_beta2]\n",
    "    ])\n",
    "    \n",
    "    return hess\n",
    "\n",
    "# Modified Newton's method\n",
    "def modified_newton(x1, x2, y, tol=1e-4, max_iter=100):\n",
    "    beta = np.zeros(2)  # Initialize beta1 and beta2\n",
    "    \n",
    "    for i in range(max_iter):\n",
    "        grad = grad_loss_function(beta, x1, x2, y)\n",
    "        hess = hessian_loss_function(beta, x1, x2, y)\n",
    "        \n",
    "        # Check if the gradient norm is small enough (stopping criterion)\n",
    "        if np.linalg.norm(grad) < tol:\n",
    "            print(f'Converged in {i} iterations.')\n",
    "            return beta\n",
    "        \n",
    "        try:\n",
    "            delta_beta = np.linalg.solve(hess, grad)\n",
    "            beta = beta - delta_beta\n",
    "        except:\n",
    "            print(f\"Singular matrix encountered at iteration {i}.\")\n",
    "            return None\n",
    "    \n",
    "    print(\"Maximum iterations reached.\")\n",
    "    return beta\n",
    "\n",
    "# Main function to load data, run optimization, and estimate beta values\n",
    "def main(csv_file_path):\n",
    "    # Load data from the given CSV file\n",
    "    x1, x2, y = load_data_from_csv(csv_file_path)\n",
    "    \n",
    "    # Apply the modified Newton method\n",
    "    beta_optimal = modified_newton(x1, x2, y)\n",
    "    \n",
    "    if beta_optimal is not None:\n",
    "        print(f\"Optimal beta values: {beta_optimal}\")\n",
    "    else:\n",
    "        print(\"Optimization failed.\")\n",
    "\n",
    "# Run the main function with the path to your CSV file\n",
    "csv_file_path = './new data.csv'  # Replace with the actual file path\n",
    "main(csv_file_path)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "omllabs",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
