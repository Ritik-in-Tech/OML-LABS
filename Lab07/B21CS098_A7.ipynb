{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import libraries\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy.optimize import minimize\n",
    "from scipy.special import expit"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "for iteration: 1\n",
      "x0 value: [-0.2  0. ]\n",
      "function evaluation: 1.4416\n",
      "gradient evaluation: [-2.432 -0.08 ]\n",
      "\n",
      "for iteration: 2\n",
      "x0 value: [ 0.91111111 -0.40444444]\n",
      "function evaluation: 1.5320591373266275\n",
      "gradient evaluation: [ 4.32153635 -2.4691358 ]\n",
      "\n",
      "for iteration: 3\n",
      "x0 value: [0.93673389 0.87681385]\n",
      "function evaluation: 0.004003032092514692\n",
      "gradient evaluation: [-0.12407226 -0.00131305]\n",
      "\n",
      "for iteration: 4\n",
      "x0 value: [0.99991704 0.99584197]\n",
      "function evaluation: 1.594382887746805e-05\n",
      "gradient evaluation: [ 0.01580119 -0.00798422]\n",
      "\n",
      "for iteration: 5\n",
      "x0 value: [0.99999934 0.99999868]\n",
      "function evaluation: 4.3188788640286794e-13\n",
      "gradient evaluation: [-1.28719683e-06 -1.35484552e-08]\n",
      "\n",
      "Solution: [0.99999934 0.99999868]\n",
      "Function value at solution: 4.3188788640286794e-13\n"
     ]
    }
   ],
   "source": [
    "def f(x):\n",
    "    return (1 - x[0])**2 + (x[1] - x[0]**2)**2\n",
    "\n",
    "def grad_f(x):\n",
    "    df_dx1 = -2 * (1 - x[0]) - 4 * x[0] * (x[1] - x[0]**2)\n",
    "    df_dx2 = 2 * (x[1] - x[0]**2)\n",
    "    return np.array([df_dx1, df_dx2])\n",
    "\n",
    "def hessian_f(x):\n",
    "    h11 = 2 - 4 * (x[1] - 3 * x[0]**2)\n",
    "    h12 = -4 * x[0]\n",
    "    h21 = h12\n",
    "    h22 = 2\n",
    "    return np.array([[h11, h12], [h21, h22]])\n",
    "\n",
    "def calculate_grad_hess(x, L):\n",
    "    return grad_f(x), hessian_f(x)\n",
    "\n",
    "def gradient(L, x):\n",
    "    return grad_f(x)\n",
    "\n",
    "def newton_method(L, A, max_iterations=2000):\n",
    "    x0 = [0, 3]\n",
    "\n",
    "    itr = 0\n",
    "    while not (np.linalg.norm(gradient(L, x0)) < 1e-4 or itr >= max_iterations):\n",
    "        grad, hess = calculate_grad_hess(x0, L)\n",
    "        try:\n",
    "            d = -np.linalg.solve(hess, grad)\n",
    "        except np.linalg.LinAlgError:\n",
    "            hess = hess + np.eye(2) * 1e-4\n",
    "            d = -np.linalg.solve(hess, grad)\n",
    "\n",
    "        x0 = x0 + d\n",
    "\n",
    "        print(f'for iteration: {itr+1}')\n",
    "        print(f'x0 value: {x0}')\n",
    "        print(f'function evaluation: {L(x0)}')\n",
    "        print(f'gradient evaluation: {gradient(L, x0)}')\n",
    "        print()\n",
    "\n",
    "        itr += 1\n",
    "\n",
    "    return x0, L(x0)\n",
    "\n",
    "\n",
    "A = np.array([[1, 0], [0, 1]])\n",
    "\n",
    "solution, final_value = newton_method(f, A)\n",
    "\n",
    "print(\"Solution:\", solution)\n",
    "print(\"Function value at solution:\", final_value)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question02\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converged in 5 iterations.\n",
      "Optimized weights: [-8.40469636e+00  1.23182298e-01  3.51637146e-02 -1.32955469e-02\n",
      "  6.18964365e-04 -1.19169898e-03  8.97009700e-02  9.45179740e-01\n",
      "  1.48690047e-02]\n"
     ]
    }
   ],
   "source": [
    "from numpy.linalg import LinAlgError\n",
    "\n",
    "data =pd.read_csv('../Lab06/diabetes2.csv') # Load the dataset\n",
    "\n",
    "# print(data.head)\n",
    "\n",
    "X = data.drop(columns='Outcome').values   # Extract features and labels\n",
    "y = data['Outcome'].values  # Labels\n",
    "y = 2 * y - 1  # Convert to {-1, 1}\n",
    "\n",
    "X = np.hstack([np.ones((X.shape[0], 1)), X]) # Add an intercept term to X\n",
    "\n",
    "# Sigmoid function\n",
    "def sigmoid(z):\n",
    "    return expit(z)\n",
    "\n",
    "# Logistic loss function\n",
    "def logistic_loss(x, X, y):\n",
    "    z = X @ x\n",
    "    return -np.sum(np.log(sigmoid(y * z)))\n",
    "\n",
    "# Gradient of the logistic loss\n",
    "def grad_logistic_loss(x, X, y):\n",
    "    z = X @ x\n",
    "    grad = -X.T @ (y * (1 - sigmoid(y * z)))\n",
    "    return grad\n",
    "\n",
    "# Hessian of the logistic loss\n",
    "def hessian_logistic_loss(x, X, y):\n",
    "    z = X @ x\n",
    "    S = sigmoid(y * z) * (1 - sigmoid(y * z))\n",
    "    H = X.T @ np.diag(S) @ X\n",
    "    return H\n",
    "\n",
    "# Modified Newton's method with logistic regression\n",
    "def modified_newton_logistic(X, y, tol=1e-4, max_iter=2000):\n",
    "    x = np.zeros(X.shape[1])  # Initialize weights\n",
    "    for i in range(max_iter):\n",
    "        grad = grad_logistic_loss(x, X, y)\n",
    "        hess = hessian_logistic_loss(x, X, y)\n",
    "\n",
    "        # Check if the gradient norm is small enough (stopping criterion)\n",
    "        if np.linalg.norm(grad) < tol:\n",
    "            print(f'Converged in {i} iterations.')\n",
    "            return x\n",
    "        \n",
    "        # Check if the Hessian is positive definite\n",
    "        try:\n",
    "            np.linalg.cholesky(hess)\n",
    "        except LinAlgError:\n",
    "            print(\"Hessian matrix is not positive definite.\")\n",
    "            return None\n",
    "        \n",
    "        # Update the parameter vector x using Newton's method\n",
    "        delta_x = np.linalg.solve(hess, grad)\n",
    "        x = x - delta_x\n",
    "\n",
    "    print(\"Maximum iterations reached.\")\n",
    "    return x\n",
    "\n",
    "# Apply the modified Newton method\n",
    "solution = modified_newton_logistic(X, y)\n",
    "\n",
    "if solution is not None:\n",
    "    print(f\"Optimized weights: {solution}\")\n",
    "else:\n",
    "    print(\"Optimization failed.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question03\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimal theta values: [5.05497752 3.68816496 6.47548203]\n",
      "Estimated purchase frequency for x = 10.8: 1.0156864089807824e+23\n"
     ]
    }
   ],
   "source": [
    "def purchase_frequency(x, theta):\n",
    "    theta1, theta2, theta3 = theta\n",
    "    return np.exp(theta1 * x) * (np.cos(theta2 * x) + np.sin(theta3 * x))\n",
    "\n",
    "def mse_loss(X, y, theta, lambda_reg=0.01):\n",
    "    y_pred = purchase_frequency(X, theta)\n",
    "    loss = np.mean((y - y_pred) ** 2)\n",
    "    reg_term = lambda_reg * np.sum(theta**2)\n",
    "    return loss + reg_term\n",
    "\n",
    "def gradient(X, y, theta):\n",
    "    theta1, theta2, theta3 = theta\n",
    "    y_pred = purchase_frequency(X, theta)\n",
    "    residual = y_pred - y\n",
    "    grad_theta1 = np.mean(2 * residual * y_pred * X)\n",
    "    grad_theta2 = np.mean(2 * residual * (-y_pred * np.sin(theta2 * X) * X))\n",
    "    grad_theta3 = np.mean(2 * residual * (y_pred * np.cos(theta3 * X) * X))\n",
    "    return np.array([grad_theta1, grad_theta2, grad_theta3])\n",
    "\n",
    "def hessian(X, theta):\n",
    "    theta1, theta2, theta3 = theta\n",
    "    y_pred = purchase_frequency(X, theta)\n",
    "    H = np.zeros((3, 3))\n",
    "    H[0, 0] = np.mean(2 * (X ** 2) * y_pred**2)\n",
    "    H[1, 1] = np.mean(2 * (X ** 2) * y_pred**2 * np.sin(theta2 * X)**2)\n",
    "    H[2, 2] = np.mean(2 * (X ** 2) * y_pred**2 * np.cos(theta3 * X)**2)\n",
    "    return H\n",
    "\n",
    "def newton_method(X, y, tol=1e-4, max_iter=100):\n",
    "    theta = np.array([0.5, 0.5, 0.5])\n",
    "    for i in range(max_iter):\n",
    "        grad = gradient(X, y, theta)\n",
    "        H = hessian(X, theta)\n",
    "        try:\n",
    "            delta_theta = np.linalg.solve(H, -grad)\n",
    "        except np.linalg.LinAlgError:\n",
    "            H = H + np.eye(3) * 1e-4\n",
    "            delta_theta = np.linalg.solve(H, -grad)\n",
    "        theta = theta + delta_theta\n",
    "        if np.linalg.norm(grad) < tol:\n",
    "            print(f\"Converged in {i+1} iterations.\")\n",
    "            break\n",
    "    return theta\n",
    "\n",
    "df = pd.read_csv('Customer Purchasing Behaviors.csv')\n",
    "X = df['loyalty_score'].values\n",
    "y = df['purchase_frequency'].values\n",
    "\n",
    "optimal_theta = newton_method(X, y)\n",
    "\n",
    "print(\"Optimal theta values:\", optimal_theta)\n",
    "\n",
    "R = 98 # last 2 digits of roll no.\n",
    "x_test = R / 10 + 1\n",
    "y_estimate = purchase_frequency(x_test, optimal_theta)\n",
    "\n",
    "print(f\"Estimated purchase frequency for x = {x_test}: {y_estimate}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question04\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converged after 2 iterations.\n",
      "Optimal beta values: [5.50988877e+08 2.91992169e+08]\n"
     ]
    }
   ],
   "source": [
    "def logistic_function(x1, x2, beta1, beta2):\n",
    "    z = beta1 * x1 + beta2 * x2\n",
    "    z_clipped = np.clip(z, -500, 500)\n",
    "    return np.exp(z_clipped) / (1 + np.exp(z_clipped))\n",
    "\n",
    "def total_error(beta, X1, X2, y):\n",
    "    beta1, beta2 = beta\n",
    "    predictions = logistic_function(X1, X2, beta1, beta2)\n",
    "    return np.sum((predictions - y) ** 2)\n",
    "\n",
    "def gradient(beta, X1, X2, y):\n",
    "    beta1, beta2 = beta\n",
    "    predictions = logistic_function(X1, X2, beta1, beta2)\n",
    "\n",
    "    grad_beta1 = 2 * np.sum((predictions - y) * predictions * (1 - predictions) * X1)\n",
    "    grad_beta2 = 2 * np.sum((predictions - y) * predictions * (1 - predictions) * X2)\n",
    "\n",
    "    return np.array([grad_beta1, grad_beta2])\n",
    "\n",
    "def hessian(beta, X1, X2, y):\n",
    "    beta1, beta2 = beta\n",
    "    predictions = logistic_function(X1, X2, beta1, beta2)\n",
    "\n",
    "    hess_beta1_beta1 = 2 * np.sum(predictions * (1 - predictions) * (1 - 2 * predictions) * X1**2)\n",
    "    hess_beta2_beta2 = 2 * np.sum(predictions * (1 - predictions) * (1 - 2 * predictions) * X2**2)\n",
    "    hess_beta1_beta2 = 2 * np.sum(predictions * (1 - predictions) * (1 - 2 * predictions) * X1 * X2)\n",
    "\n",
    "    return np.array([[hess_beta1_beta1, hess_beta1_beta2], [hess_beta1_beta2, hess_beta2_beta2]])\n",
    "\n",
    "def modified_newton_method(X1, X2, y, beta_init, max_iters=1000, tol=1e-4):\n",
    "    converged = False\n",
    "    iteration = 0\n",
    "    beta = beta_init.copy()\n",
    "    while not converged and iteration < max_iters:\n",
    "        grad = gradient(beta, X1, X2, y)\n",
    "        hess = hessian(beta, X1, X2, y)\n",
    "\n",
    "        hess += np.eye(2) * 1e-4\n",
    "\n",
    "        delta_beta = np.linalg.solve(hess, -grad)\n",
    "        beta += delta_beta\n",
    "\n",
    "        if np.linalg.norm(delta_beta) < tol:\n",
    "            converged = True\n",
    "            print(f\"Converged after {iteration + 1} iterations.\")\n",
    "\n",
    "        iteration += 1\n",
    "\n",
    "    if not converged:\n",
    "        print(f\"Did not converge within {max_iters} iterations.\")\n",
    "\n",
    "    return beta\n",
    "\n",
    "data = pd.read_csv('new data.csv')\n",
    "\n",
    "Y = data['Y'].values\n",
    "X1 = data['x1'].values\n",
    "X2 = data['x2'].values\n",
    "\n",
    "beta_init = np.array([0.0, 0.0])\n",
    "\n",
    "beta_optimal = modified_newton_method(X1, X2, Y, beta_init)\n",
    "print(f\"Optimal beta values: {beta_optimal}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "omllabs",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
